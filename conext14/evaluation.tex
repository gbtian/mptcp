%!TEX root =conext14.tex
\section{Performance Evaluation}
\label{sec:evaluation}

To evaluate the performance of our proposed system, we implemented our multipath IP in Ubuntu under Linux kernel $3.12.1$ and install the prototype on two desktops. Both desktops are connected directly to one router without any middlebox. At each desktop, two NICs working with $100Mbps$ capacity are installed which means that there are totally $4$ paths and the throughput upper bound is $200MBps$ between the two nodes. Also, to evaluate our prototype under multiple scenarios, for some experiments, we limit the capacity of one NIC card on the client to $30Mbps$ with built-in netem feature in Ubuntu. Also, we replace one NIC card on the client with a wireless NIC to evaluate our system for wireless environment.

Besides the controlled lab experiments, we also evaluate MPIP on the Internet. We connect the client to a customized node in Emulab. According to our test, the capacity between our client in the lab and the server in Emulab is about $5Mbps$, so we limit the bandwidth of both NICs on the client to be $2Mbps$. Because there is only one NIC that connects to Internet on the Emulab card, there are only $2$ paths in this case.

In all experiments, we try to keep the configuration of each node unchanged after installation. We don't any special configuration to the default TCP congestion control algorithm, neither we do any optimization to TCP parameters to squeeze out all possible throughput. Beside enabling MPIP feature at he kernel source tree, to make MPIP feasible, we need to configure IP routing rules to route incoming packets to a proper NIC as in MPTCP.

Except specific experiments that can only be applied to MPIP like UDP experiment and customization MPIP routing, we try to do side-by-side comparison with MPTCP for TCP connections. We will figure out how these two features work independently and together as stated in Section~\ref{sec:together};

For all throughput related experiments, we used iperf3 to transmit packets from the client to the server.

%We modify an open source software \bf{Simple Traffic Monitor}\cite{simon01} to record the real time traffic that goes through each NIC.

%For mobile devices, we implement this feature in Google Nexus $4$ with Android $4.01$.

\subsection{TCP/UDP throughput enhancement}
\label{sec:tcp}

In this section, we try to verify that our system can achieve high throughput in both TCP and UDP scenarios. 

\begin{figure*}[htb]
\centering{
\subfigure{\includegraphics[width=0.33\linewidth]{fig/out_of_order_wo_sack.eps}}
\subfigure{\includegraphics[width=0.33\linewidth]{fig/out_of_order_w_sack.eps}}
\subfigure{\includegraphics[width=0.33\linewidth]{fig/clock.eps}}
\subfigure{\includegraphics[width=0.33\linewidth]{fig/implementation.eps}}
\subfigure{\includegraphics[width=0.33\linewidth]{fig/no_limit.eps}}
\subfigure{\includegraphics[width=0.33\linewidth]{fig/no_limit_tp_comp.eps}}
\subfigure{\includegraphics[width=0.33\linewidth]{fig/no_limit_qd_comp.eps}}
\subfigure{\includegraphics[width=0.33\linewidth]{fig/limit.eps}}
\subfigure{\includegraphics[width=0.33\linewidth]{fig/limit_tp_comp.eps}}
\subfigure{\includegraphics[width=0.33\linewidth]{fig/limit_qd_comp.eps}}
\subfigure{\includegraphics[width=0.33\linewidth]{fig/delay_1.eps}}
\subfigure{\includegraphics[width=0.33\linewidth]{fig/delay_tp_comp.eps}}
\subfigure{\includegraphics[width=0.33\linewidth]{fig/delay_qd_com.eps}}
\subfigure{\includegraphics[width=0.33\linewidth]{fig/wireless.eps}}
\subfigure{\includegraphics[width=0.33\linewidth]{fig/wireless_tp_comp.eps}}
}
\caption{Side-by-side comparison for no limit}
\label{fig.no_limit}
\end{figure*}

\begin{figure*}[htb]
\centering{
\subfigure{\includegraphics[width=0.33\linewidth]{fig/wireless_qd_comp.eps}}
\subfigure{\includegraphics[width=0.33\linewidth]{fig/routing_ack.eps}}
\subfigure{\includegraphics[width=0.33\linewidth]{fig/emulab.eps}}
\subfigure{\includegraphics[width=0.33\linewidth]{fig/emulab_tp_comp.eps}}
\subfigure{\includegraphics[width=0.33\linewidth]{fig/emulab_qd_comp.eps}}
\subfigure{\includegraphics[width=0.33\linewidth]{fig/pair_limit.eps}}
\subfigure{\includegraphics[width=0.33\linewidth]{fig/pair_limit_tp_comp.eps}}
\subfigure{\includegraphics[width=0.33\linewidth]{fig/pair_limit_qd_comp.eps}}
\subfigure{\includegraphics[width=0.33\linewidth]{fig/twopath.eps}}
\subfigure{\includegraphics[width=0.33\linewidth]{fig/twopaths_tp_comp.eps}}
\subfigure{\includegraphics[width=0.33\linewidth]{fig/twopaths_qd_comp.eps}}
\subfigure{\includegraphics[width=0.33\linewidth]{fig/skype_tp_comp.eps}}
\subfigure{\includegraphics[width=0.33\linewidth]{fig/skype_delay_comp.eps}}

}
\caption{Side-by-side comparison for no limit}
\label{fig.no_limit}
\end{figure*}


\subsection{Out-of-order evaluation for TCP}
\label{sec:reoder}

To evaluation the effect of out-of-order process in our prototype, we design following experiment. We replace one NIC with wireless NIC, then there will still be $4$ paths. For all the paths, we assign the same path weight which means that every path is assigned the same amount of packets. In this scenario, $50\%$ percent of packets will be sent throughput the wireless NIC. The difference between propagation delay of the wireless path and wired path is large enough to generate a substantial number of out-of-order packets. By enabling and disabling out-of-order functionality, we get Figure~\ref{fig.outoforder}.


\subsection{Skype voice call improvement}
\label{sec:skype}

In Section~\ref{sec:resp}, we mentioned that Skype can benefit from our customized routing for responsiveness. Skype calls has higher requirement on the responsiveness of audio packets. According to our experiments, Skype audio packets are generally less than $200$ bytes. In Table~\ref{tb.route}, for packets that is smaller than $200$ bytes, responsiveness consideration will be the first priority. 

To show how this rule influence Skype audio streaming, we change the delay of NIC cards using the netem package in Ubuntu. On one side of the Skype call, we have two NIC cards as stated above. We ran the experiment for $360$ seconds with $3$ stages. In the first $120$ seconds, there is no manually added delay on each NIC card, we add additional $120ms$ delay to the first NIC card in the following $120$ seconds, finally for the last $120$ seconds, we add $120ms$ delay to the second NIC card. We did our Skype audio experiment without video content while most packets on the connection are audio packets except some control packets. Figure~\ref{fig.skype} shows the packets routing adaptation as the delay value changes. By this adaptation, we can make sure that the audio streaming will always choose the path that has lower delay to keep the best user experience even other paths have the same throughput.


\subsection{Smooth connection switch}
\label{sec:switch}

In Figure~\ref{fig.switch}, we verify that smooth switch between different NIC cards works perfectly over our MPIP implementation by doing an IPERF TCP experiment. We do a side-by-side comparison between MPIP and MPTCP. We also divide the experiment into $3$ sections with $120$ seconds for each section. On the client side of the connection, there are two NIC cards. In the first $120$ seconds, both NIC cards work synchronously, then we disable one of them for $120$ seconds, and during the last $120$ seconds, we enable back the NIC card. The result shows that our MPIP system can follow this on/off process perfectly with stable throughput. For MPTCP, as shown in previous experiments, MPTCP has higher fluctuation than MPIP even MPTCP can achieve higher throughput than MPIP. This happens because in MPTCP, there are more than one TCP connections ($4$ in this experiment), and each connection has its own congestion window. Although congestion control in MPTCP is coupled among different connections, fluctuation have more chances to happen with $4$ relatively independent congestion windows. But in MPIP, one one TCP connection is constructed for each session which means that there is only one congestion windows. In this case, the throughput is more consistent than that of MPTCP.